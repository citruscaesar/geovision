{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "import tarfile\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>size_in_megabytes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cdl</th>\n",
       "      <td>2000</td>\n",
       "      <td>6.311417e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corine</th>\n",
       "      <td>11000</td>\n",
       "      <td>3.472013e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enmap</th>\n",
       "      <td>538927</td>\n",
       "      <td>3.403156e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlcd</th>\n",
       "      <td>15000</td>\n",
       "      <td>4.733562e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  size_in_megabytes\n",
       "dataset                             \n",
       "cdl          2000       6.311417e+01\n",
       "corine      11000       3.472013e+02\n",
       "enmap      538927       3.403156e+06\n",
       "nlcd        15000       4.733562e+02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df = pd.concat([pd.read_csv(\"spectral_earth_table.csv\"), pd.read_csv(\"spectral_earth_table_part_6+.csv\")])\n",
    "#df = df.drop(columns = [\"Processing Time\"])\n",
    "#df = df.rename(columns = {\"Filename\": \"filename\", \"Size\": \"size_in_megabytes\", \"Modified Time\": \"last_modified\"})\n",
    "#df = df.drop_duplicates(subset = [\"filename\"])\n",
    "#df[\"dataset\"] = df[\"filename\"].apply(lambda x: x.split(\"/\")[-3])\n",
    "#df[\"filename\"] = df[\"filename\"].apply(lambda x: '/'.join(x.split(\"/\")[-2:]))\n",
    "#df[\"size_in_megabytes\"] = df[\"size_in_megabytes\"] / 1024**2\n",
    "#df = df.sort_values(by = \"filename\").reset_index(drop = True)\n",
    "#df.to_hdf(\"metadata.h5\", mode = \"w\", key = \"index\", format = \"fixed\", complevel=9, complib=\"zlib\")\n",
    "#df.to_csv(\"spectral_earth.csv\")\n",
    "\n",
    "df = pd.read_hdf(\"metadata.h5\", key = \"index\")\n",
    "\n",
    "display(df.groupby(\"dataset\").agg({\"filename\": \"count\", \"size_in_megabytes\": \"sum\"}))\n",
    "cdl_df, corine_df, enmap_df, nlcd_df = [d.copy().drop(columns = \"dataset\").reset_index(drop = True) for _, d in df.groupby(\"dataset\", sort = True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extraction_set(df: pd.DataFrame, dataset: str) -> set:\n",
    "    extraction_set = set()\n",
    "    for filename in df[\"filename\"]:\n",
    "        extraction_set.add(f\"spectral_earth/enmap/{filename}\")\n",
    "        extraction_set.add(f\"spectral_earth/{dataset}/{filename}\")\n",
    "    return extraction_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archive_total_size(parts_list):\n",
    "    \"\"\"\n",
    "    Calculate total size of all archive parts\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    for part in parts_list:\n",
    "        try:\n",
    "            size = os.path.getsize(part)\n",
    "            total_size += size\n",
    "        except OSError as e:\n",
    "            print(f\"Warning: Couldn't read size of part {i}: {e}\")\n",
    "    return total_size\n",
    "\n",
    "def list_files_from_split_archive(parts_list, start_part, output_csv):\n",
    "    \"\"\"\n",
    "    List files from a split tar.gz archive starting from a specific part,\n",
    "    with progress tracking and CSV logging.\n",
    "    \n",
    "    Parameters:\n",
    "    archive_prefix: Base name of the split archive (e.g., 'dataset.tar.gz.')\n",
    "    start_part: Which part to start listing from (1-based indexing)\n",
    "    total_parts: Total number of split parts\n",
    "    output_csv: Path to output CSV file\n",
    "    \"\"\"\n",
    "    # Get total size for progress bar\n",
    "    total_size = int(3.57e12)\n",
    "    \n",
    "    # Command to concatenate ALL parts\n",
    "    cat_command = ['cat', *parts_list]\n",
    "    \n",
    "    # Calculate target offset\n",
    "    target_offset = (start_part - 1) * (300 * 1024 * 1024 * 1024)  # 300GB in bytes\n",
    "    bytes_processed = 0\n",
    "    started_listing = False\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Setup CSV file\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Filename', 'Size', 'Modified Time', 'Processing Time'])\n",
    "        \n",
    "        try:\n",
    "            # Open a pipe to read the concatenated data\n",
    "            process = subprocess.Popen(cat_command, stdout=subprocess.PIPE)\n",
    "            \n",
    "            # Create a gzip decompressor\n",
    "            decompressor = gzip.GzipFile(fileobj=process.stdout, mode='rb')\n",
    "            \n",
    "            # Create progress bar\n",
    "            pbar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "            \n",
    "            # Create a tar file object\n",
    "            with tarfile.open(fileobj=decompressor, mode='r|') as tar:\n",
    "                # Iterate through all members\n",
    "                for member in tar:\n",
    "                    current_time = time.time()\n",
    "                    elapsed = current_time - start_time\n",
    "                    \n",
    "                    # Update progress\n",
    "                    bytes_processed += member.size + 512  # Add header size\n",
    "                    pbar.update(member.size + 512)\n",
    "                    \n",
    "                    # Start listing once we've passed our target offset\n",
    "                    if bytes_processed >= target_offset:\n",
    "                        started_listing = True\n",
    "                    \n",
    "                    if started_listing and member.isfile():\n",
    "                        # Log to console\n",
    "                        # print(f\"\\nFile: {member.name}\")\n",
    "                        # print(f\"Size: {member.size:,} bytes\")\n",
    "                        # print(f\"Modified: {datetime.fromtimestamp(member.mtime)}\")\n",
    "                        \n",
    "                        # Log to CSV\n",
    "                        csvwriter.writerow([\n",
    "                            member.name,\n",
    "                            member.size,\n",
    "                            datetime.fromtimestamp(member.mtime).isoformat(),\n",
    "                            f\"{elapsed:.2f}\"\n",
    "                        ])\n",
    "                        csvfile.flush()  # Ensure immediate write\n",
    "                        \n",
    "            pbar.close()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        finally:\n",
    "            # Clean up\n",
    "            process.stdout.close()\n",
    "            process.wait()\n",
    "            \n",
    "        # Print summary\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nTotal processing time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "def estimate_offset(part_size_gb, start_part):\n",
    "    \"\"\"\n",
    "    Print estimated offset information\n",
    "    \"\"\"\n",
    "    offset_gb = (start_part - 1) * part_size_gb\n",
    "    print(f\"Will process full archive but only list files after {offset_gb}GB offset\")\n",
    "\n",
    "def extract_files_from_split_archive(parts_list: list, subset_to_extract: set):\n",
    "    \"\"\"\n",
    "    Extract specific files from a split tar.gz archive.\n",
    "    \n",
    "    Parameters:\n",
    "    archive_prefix: Base name of the split archive (e.g., 'dataset.tar.gz.')\n",
    "    total_parts: Total number of split parts\n",
    "    output_dir: Directory to extract files to\n",
    "    file_pattern: Regex pattern to match files to extract (optional)\n",
    "    max_files: Maximum number of files to extract (optional)\n",
    "    \"\"\"\n",
    "    # Get total size for progress bar\n",
    "    # total_size = get_archive_total_size(parts_list)    \n",
    "    total_size = int(3.57e12)\n",
    "\n",
    "    # Command to concatenate ALL parts\n",
    "    cat_command = ['cat', *parts_list]\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    #output_dir = \"/home/sambhav/datasets/spectral_earth/imagefolder\"\n",
    "    output_dir = \"/run/media/sambhav/StorageHDD_2/datasets/spectral_earth/imagefolder\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup counters and tracking\n",
    "    bytes_processed = 0\n",
    "    files_extracted = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Setup logging\n",
    "    log_file = os.path.join(output_dir, 'extraction_log.csv')\n",
    "    with open(log_file, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(['Filename', 'Size', 'Modified Time', 'Extraction Time', 'Status'])\n",
    "        \n",
    "        try:\n",
    "            # Open a pipe to read the concatenated data\n",
    "            process = subprocess.Popen(cat_command, stdout=subprocess.PIPE)\n",
    "            \n",
    "            # Create a gzip decompressor\n",
    "            decompressor = gzip.GzipFile(fileobj=process.stdout, mode='rb')\n",
    "            \n",
    "            # Create progress bar\n",
    "            pbar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "            \n",
    "            # Create a tar file object\n",
    "            with tarfile.open(fileobj=decompressor, mode='r|') as tar:\n",
    "                for member in tar:\n",
    "                    current_time = time.time()\n",
    "                    elapsed = current_time - start_time\n",
    "                    \n",
    "                    # Update progress\n",
    "                    bytes_processed += member.size + 512  # Add header size\n",
    "                    pbar.update(member.size + 512)\n",
    "                    \n",
    "                    if member.isfile() and member.name in subset_to_extract:\n",
    "                        try:\n",
    "                            # Extract the file\n",
    "                            tar.extract(member, output_dir)\n",
    "                            files_extracted += 1\n",
    "                            \n",
    "                            # Log success\n",
    "                            csvwriter.writerow([\n",
    "                                member.name,\n",
    "                                member.size,\n",
    "                                datetime.fromtimestamp(member.mtime).isoformat(),\n",
    "                                f\"{elapsed:.2f}\",\n",
    "                                \"Success\"\n",
    "                            ])\n",
    "                            \n",
    "                            # Print progress\n",
    "                            # print(f\"\\nExtracted: {member.name}, Size: {member.size:,} bytes\")\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            # Log failure\n",
    "                            csvwriter.writerow([\n",
    "                                member.name,\n",
    "                                member.size,\n",
    "                                datetime.fromtimestamp(member.mtime).isoformat(),\n",
    "                                f\"{elapsed:.2f}\",\n",
    "                                f\"Failed: {str(e)}\"\n",
    "                            ])\n",
    "                            print(f\"\\nError extracting {member.name}: {str(e)}\")\n",
    "                            \n",
    "                    csvfile.flush()  # Ensure immediate write\n",
    "                        \n",
    "            pbar.close()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            print(f\"Bytes processed when error occurred: {bytes_processed:,}\")\n",
    "        finally:\n",
    "            # Clean up\n",
    "            process.stdout.close()\n",
    "            process.wait()\n",
    "            \n",
    "        # Print summary\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nExtraction complete:\")\n",
    "        print(f\"Total files extracted: {files_extracted}\")\n",
    "        print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Extraction log saved to: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFileReader:\n",
    "    \"\"\"Reads multiple files sequentially as a single stream while ensuring valid tar alignment.\"\"\"\n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "        self.file_iter = iter(self.filenames)\n",
    "        self.current_file = None\n",
    "        self._open_next_file()\n",
    "\n",
    "    def _open_next_file(self):\n",
    "        \"\"\"Open the next available file in sequence.\"\"\"\n",
    "        if self.current_file:\n",
    "            self.current_file.close()\n",
    "        try:\n",
    "            next_file = next(self.file_iter)\n",
    "            print(f\"Opening archive part: {next_file}\")\n",
    "            self.current_file = open(next_file, \"rb\")\n",
    "        except StopIteration:\n",
    "            self.current_file = None\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        \"\"\"Read bytes from the current file, switching to the next file when needed.\"\"\"\n",
    "        if not self.current_file:\n",
    "            return b\"\"  # No more data\n",
    "        \n",
    "        data = self.current_file.read(size)\n",
    "        if not data:  # End of current file, move to next\n",
    "            self._open_next_file()\n",
    "            return self.read(size)  # Recursive call to continue reading\n",
    "        return data\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the last opened file.\"\"\"\n",
    "        if self.current_file:\n",
    "            print(\"Closing final file.\")\n",
    "            self.current_file.close()\n",
    "\n",
    "def list_tar_files(part_files, output_file=\"file_list.txt\", start_index=0):\n",
    "    \"\"\"\n",
    "    Lists the contents of a split .tar archive without loading the entire archive into memory,\n",
    "    ensuring alignment with a valid tar header.\n",
    "\n",
    "    :param part_files: List of tar parts in correct order.\n",
    "    :param output_file: Path to save the file listing.\n",
    "    :param start_index: The index of the first file to include in the listing (0-based).\n",
    "    \"\"\"\n",
    "    if start_index >= len(part_files):\n",
    "        print(\"Error: Start index is out of range.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing archives starting from index {start_index} ({part_files[start_index]} onwards)...\")\n",
    "    \n",
    "    # Open the output file\n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        print(f\"Writing file list to {output_file}\")\n",
    "        \n",
    "        # Open all parts as a continuous stream, ensuring we start at a valid tar header\n",
    "        reader = MultiFileReader(part_files)\n",
    "\n",
    "        # TODO: Try getting the member list using tar.getmembers() ?\n",
    "\n",
    "        with tarfile.open(fileobj=reader, mode=\"r|*\") as tar:\n",
    "            skipped = 0\n",
    "            for member in tar:\n",
    "                if skipped < start_index:\n",
    "                    skipped += 1\n",
    "                    continue  # Skip until we reach the desired start index\n",
    "                \n",
    "                # Log metadata\n",
    "                archive_part = part_files[min(skipped, len(part_files) - 1)]\n",
    "                file_info = f\"{member.name}, Size: {member.size} bytes, Archive Part: {archive_part}\\n\"\n",
    "                out_file.write(file_info)\n",
    "                # print(f\"Found file: {file_info.strip()}\")  # Log output\n",
    "    \n",
    "    print(\"File listing completed.\")\n",
    "\n",
    "def extract_file_from_tar(part_files, target_file, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Extracts a specific file from a split tar archive.\n",
    "\n",
    "    :param part_files: List of tar parts in correct order.\n",
    "    :param target_file: The specific file to extract.\n",
    "    :param output_dir: Directory to extract the file into.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting {target_file} from archive...\")\n",
    "    \n",
    "    # Open the multi-part tar archive as a continuous stream\n",
    "    reader = MultiFileReader(part_files)\n",
    "    with tarfile.open(fileobj=reader, mode=\"r|*\") as tar:\n",
    "        for member in tar:\n",
    "            if member.name == target_file:\n",
    "                print(f\"Extracting: {member.name} -> {output_dir}\")\n",
    "                tar.extract(member, path=output_dir)\n",
    "                print(\"Extraction complete.\")\n",
    "                return\n",
    "    \n",
    "    print(\"File not found in archive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3.57T/3.57T [8:26:52<00:00, 117MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extraction complete:\n",
      "Total files extracted: 55419\n",
      "Total processing time: 30412.64 seconds\n",
      "Extraction log saved to: /run/media/sambhav/StorageHDD_2/datasets/spectral_earth/imagefolder/extraction_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "part_files = [\n",
    "    \"/run/media/sambhav/StorageSSD_1/datasets/spectral_earth/spectral_earth_part_1.tar.gz\",\n",
    "    \"/run/media/sambhav/StorageSSD_1/datasets/spectral_earth/spectral_earth_part_2.tar.gz\",\n",
    "    \"/run/media/sambhav/StorageHDD_2/datasets/spectral_earth/spectral_earth_part_3.tar.gz\",\n",
    "    \"/run/media/sambhav/StorageHDD_2/datasets/spectral_earth/spectral_earth_part_4.tar.gz\",\n",
    "    \"/run/media/sambhav/StorageHDD_1/datasets/spectral_earth/spectral_earth_part_5.tar.gz\",\n",
    "    \"/run/media/sambhav/StorageHDD_1/datasets/spectral_earth/spectral_earth_part_6.tar.gz\",\n",
    "    \"/home/sambhav/datasets/spectral_earth/spectral_earth_part_7.tar.gz\",\n",
    "    \"/home/sambhav/datasets/spectral_earth/spectral_earth_part_8.tar.gz\",\n",
    "    \"/run/media/sambhav/StorageSSD_1/datasets/spectral_earth/spectral_earth_part_9.tar.gz\",\n",
    "]\n",
    "extract_files_from_split_archive(part_files, get_extraction_set(cdl_df, \"cdl\").union(get_extraction_set(nlcd_df, \"nlcd\")).union(get_extraction_set(corine_df, \"corine\")))\n",
    "#list_tar_files(part_files, \"spectral_earth_files_part_9\", 8)\n",
    "#list_files_from_split_archive(part_files, 6, \"spectral_earth_table_part_6+.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
